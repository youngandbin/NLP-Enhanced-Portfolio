{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YOUNGBIN\\anaconda3\\envs\\DB_GAPS\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import FinanceDataReader as fdr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(14)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn, optim\n",
    "import torch.nn.functional as F\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "AAPL = fdr.DataReader('AAPL', '2021')\n",
    "GOOGL = fdr.DataReader('GOOGL', '2021')\n",
    "MSFT = fdr.DataReader('MSFT', '2021')\n",
    "NFLX = fdr.DataReader('NFLX', '2021')\n",
    "TSLA = fdr.DataReader('TSLA', '2021')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a dataframe for close prices\n",
    "df_close = pd.DataFrame({'AAPL': AAPL['Close'],\n",
    "                         'GOOGL': GOOGL['Close'],\n",
    "                         'MSFT': MSFT['Close'],\n",
    "                         'NFLX': NFLX['Close'],\n",
    "                         'TSLA': TSLA['Close']})\n",
    "# Making a dataframe for log return\n",
    "df_logret = np.log(df_close / df_close.shift(1))\n",
    "df_logret = df_logret.dropna()\n",
    "# calculate mu\n",
    "mu_df = df_logret.mean() * len(df_logret)\n",
    "mu = mu_df.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Sigma\n",
    "\n",
    "Sigma_df = df_logret.cov() * len(df_logret)\n",
    "Sigma = Sigma_df.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word embeddings (random)\n",
    "\n",
    "word_embeds = np.random.rand(5, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "- Sigma_embeddings\n",
    "    - non-diagonal: 두 주식의 text embedding 사이의 Minkowski distance \n",
    "    - diagonal: 주식의 과거 시계열의 분산\n",
    "        - look-back window of 4 months\n",
    "- mu\n",
    "    - input: merge text embeddings and historical returns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "\n",
    "    def __init__(self, device, word_embeds, lambda_:float, Sigma_original) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            word_embeds: vector representations of stocks\n",
    "            lambda_: weights between original cov and new cov (hyperparameter)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.word_embeds = word_embeds\n",
    "        self.lambda_ = lambda_\n",
    "        self.Sigma_original = Sigma_original\n",
    "        self.n = Sigma_original.shape[0] # 주식 개수\n",
    "        self.d = word_embeds.shape[1] # 임베딩 차원\n",
    "    \n",
    "    def get_Sigma_embeddings(self):\n",
    "        Sigma_embeddings = pairwise_distances(self.word_embeds, metric='minkowski') # (n x n)\n",
    "        diagonal = self.Sigma_original.diagonal() # (n x 1) \n",
    "        np.fill_diagonal(Sigma_embeddings, diagonal)\n",
    "        self.Sigma_embeddings = Sigma_embeddings\n",
    "\n",
    "    def get_Sigma_new(self):\n",
    "        return self.lambda_*self.Sigma_original + (1-self.lambda_)*self.Sigma_embeddings\n",
    "\n",
    "    def get_mu(self, DNN_trained, input):\n",
    "        DNN_trained.eval()\n",
    "        with torch.no_grad():\n",
    "            output = DNN_trained.forward(input)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 106)\n",
    "        self.fc2 = nn.Linear(106, 53)\n",
    "        self.fc3 = nn.Linear(53, 26)\n",
    "        self.fc4 = nn.Linear(26, 21)\n",
    "\n",
    "        # Dropout module with 0.2 drop probability\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Now with dropout\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "\n",
    "        # output so no dropout here\n",
    "        # x = F.log_softmax(self.fc4(x), dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "def train():\n",
    "\n",
    "    model = Classifier()\n",
    "\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "    epochs = 1\n",
    "    steps = 0\n",
    "\n",
    "    train_losses, test_losses = [], []\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0\n",
    "        for images, labels in trainloader:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            log_ps = model(images)\n",
    "            loss = criterion(log_ps, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        else:\n",
    "            test_loss = 0\n",
    "            accuracy = 0\n",
    "\n",
    "            # Turn off gradients for validation, saves memory and computations\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                for images, labels in testloader:\n",
    "                    log_ps = model(images)\n",
    "                    test_loss += criterion(log_ps, labels)\n",
    "\n",
    "                    ps = torch.exp(log_ps)\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    equals = top_class == labels.view(*top_class.shape)\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            train_losses.append(running_loss/len(trainloader))\n",
    "            test_losses.append(test_loss/len(testloader))\n",
    "\n",
    "            print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "                \"Training Loss: {:.3f}.. \".format(running_loss/len(trainloader)),\n",
    "                \"Test Loss: {:.3f}.. \".format(test_loss/len(testloader)),\n",
    "                \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))\n",
    "\n",
    "    return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10496193, 0.97825637, 0.99331543, 0.94968438, 0.98139989],\n",
       "       [0.97825637, 0.10917826, 0.95832465, 1.02096128, 0.95982202],\n",
       "       [0.99331543, 0.95832465, 0.09099032, 1.00257978, 1.01041754],\n",
       "       [0.94968438, 1.02096128, 1.00257978, 0.45598878, 1.02048379],\n",
       "       [0.98139989, 0.95982202, 1.01041754, 1.02048379, 0.47934328]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyModel = MyModel(word_embeds=word_embeds, device=torch.device(\"cuda\"), lambda_=0.8, Sigma_original=Sigma)\n",
    "\n",
    "MyModel.get_Sigma_embeddings()\n",
    "\n",
    "Sigma_new = MyModel.get_Sigma_new()\n",
    "Sigma_new"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "37194c3c690497ab80e47e07953a810565eabaf73d72714da06e07c6eb322f18"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('DB_GAPS')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
